{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ue04SNNyyqNM"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai faiss-cpu sentence-transformers transformers tiktoken langchain-community\n",
        "\n",
        "import os\n",
        "import json\n",
        "import faiss\n",
        "import numpy as np\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_dir = \"/content/drive/MyDrive/Capstone/MATH/train\"\n",
        "\n",
        "import glob\n",
        "json_files = glob.glob(base_dir + \"/*/*.json\")\n",
        "print(f\"Total JSON files: {len(json_files)}\")\n",
        "\n",
        "for file_path in json_files[:5]:\n",
        "    print(file_path)\n",
        "\n",
        "import re\n",
        "from sympy import sympify\n",
        "\n",
        "def split_content(content):\n",
        "    math_pattern = r\"(\\$.*?\\$)\"\n",
        "    text_parts = re.split(math_pattern, content)\n",
        "    chunks = []\n",
        "    for part in text_parts:\n",
        "        if re.match(math_pattern, part):\n",
        "            chunks.append({\"type\": \"math\", \"content\": part})\n",
        "        elif part.strip():\n",
        "            chunks.append({\"type\": \"text\", \"content\": part.strip()})\n",
        "    return chunks\n",
        "\n",
        "def split_text_chunks(content, chunk_size=500, chunk_overlap=50):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    return text_splitter.split_text(content)\n",
        "\n",
        "!pip install transformers torch\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "mathbert_model_name = \"tbs17/MathBERT\"\n",
        "tokenizer_mathbert = AutoTokenizer.from_pretrained(mathbert_model_name)\n",
        "tokenizer_mathbert.pad_token = tokenizer_mathbert.eos_token\n",
        "model_mathbert = AutoModel.from_pretrained(mathbert_model_name)\n",
        "\n",
        "def generate_embedding_mathbert(content):\n",
        "    inputs = tokenizer_mathbert(content, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model_mathbert(**inputs)\n",
        "    embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
        "    return embedding\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "import glob\n",
        "import json\n",
        "\n",
        "dimension = 768\n",
        "faiss_index = faiss.IndexFlatL2(dimension)\n",
        "embedding_map = {}\n",
        "\n",
        "\n",
        "for json_file in json_files:\n",
        "    with open(json_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    if not isinstance(data, dict):\n",
        "        print(f\"Error: {json_file} does not contain a valid dictionary. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    problem = data.get(\"problem\", \"No problem found\")\n",
        "    solution = data.get(\"solution\", \"No solution found\")\n",
        "\n",
        "    if problem and solution:\n",
        "        print(f\"Processing file: {json_file}\")\n",
        "        print(f\"Problem: {problem}\")\n",
        "        print(f\"Solution: {solution}\")\n",
        "\n",
        "        problem_embedding = generate_embedding_mathbert(problem)\n",
        "        solution_embedding = generate_embedding_mathbert(solution)\n",
        "\n",
        "        faiss_index.add(np.array([problem_embedding]))\n",
        "        embedding_map[len(embedding_map)] = {\"type\": \"problem\", \"content\": problem}\n",
        "\n",
        "        faiss_index.add(np.array([solution_embedding]))\n",
        "        embedding_map[len(embedding_map)] = {\"type\": \"solution\", \"content\": solution}\n",
        "\n",
        "print(f\"FAISS index contains {len(embedding_map)} embeddings.\")\n",
        "\n",
        "num_embeddings_faiss = faiss_index.ntotal\n",
        "print(f\"Total embeddings in FAISS: {num_embeddings_faiss}\")\n",
        "\n",
        "num_embeddings_map = len(embedding_map)\n",
        "print(f\"Total entries in embedding map: {num_embeddings_map}\")\n",
        "\n",
        "expected_count = 7500 * 2\n",
        "if num_embeddings_faiss == expected_count and num_embeddings_map == expected_count:\n",
        "    print(\"All files successfully processed and indexed!\")\n",
        "else:\n",
        "    print(f\"Discrepancy detected. Expected: {expected_count}, Found: {num_embeddings_faiss}\")\n",
        "\n",
        "import pickle\n",
        "import faiss\n",
        "\n",
        "faiss.write_index(faiss_index, \"faiss_index.idx\")\n",
        "\n",
        "with open(\"embedding_map.pkl\", \"wb\") as f:\n",
        "    pickle.dump(embedding_map, f)\n",
        "\n",
        "print(\"FAISS index and embedding map saved!\")\n",
        "\n",
        "!cp /content/faiss_index.idx /content/drive/MyDrive/Capstone/\n",
        "!cp /content/embedding_map.pkl /content/drive/MyDrive/Capstone/\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pickle\n",
        "import faiss\n",
        "\n",
        "faiss_index = faiss.read_index(\"/content/drive/MyDrive/Capstone/faiss_index.idx\")\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Capstone/embedding_map.pkl\", \"rb\") as f:\n",
        "    embedding_map = pickle.load(f)\n",
        "\n",
        "print(\"FAISS index and embedding map loaded successfully!\")\n",
        "\n",
        "print(f\"Total embeddings in FAISS: {faiss_index.ntotal}\")\n",
        "print(f\"Total entries in embedding map: {len(embedding_map)}\")\n",
        "\n",
        "pip install transformers accelerate\n",
        "\n",
        "from google.colab import userdata\n",
        "userdata.get('Hugging_face')\n",
        "\n",
        "!pip install --upgrade transformers\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
        "import numpy as np\n",
        "import faiss\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "\n",
        "gpt_neo_model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
        "tokenizer_gpt = AutoTokenizer.from_pretrained(gpt_neo_model_name)\n",
        "tokenizer_gpt.pad_token = tokenizer_gpt.eos_token\n",
        "gpt_neo_model = AutoModelForCausalLM.from_pretrained(gpt_neo_model_name)\n",
        "\n",
        "\n",
        "\n",
        "def retrieve(query, k=3, max_context_length=1500):\n",
        "    query_embedding = generate_embedding_mathbert(query)\n",
        "    distances, indices = faiss_index.search(np.array([query_embedding]), k)\n",
        "    results = [embedding_map[i] for i in indices[0]]\n",
        "\n",
        "    combined_context = \"\\n\".join([item[\"content\"] for item in results])\n",
        "    return combined_context[:max_context_length]\n",
        "\n",
        "template = \"\"\"\n",
        "You are a math tutor. Use the following retrieved context to answer the question.\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Answer:\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "def rag_pipeline(query):\n",
        "    # Retrieve relevant contexts\n",
        "    retrieved_contexts = retrieve(query, k=3)\n",
        "    context_text = \"\\n\".join([retrieved_contexts])\n",
        "\n",
        "\n",
        "    input_text = prompt.format(context=context_text, question=query)\n",
        "    print(f\"Input text length (characters): {len(input_text)}\")\n",
        "\n",
        "    inputs = tokenizer_gpt(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=2048\n",
        "    )\n",
        "    print(f\"Tokenized input length (tokens): {inputs['input_ids'].shape[1]}\")\n",
        "\n",
        "    outputs = gpt_neo_model.generate(\n",
        "        **inputs,\n",
        "        max_length=200,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    return tokenizer_gpt.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "query = \"What is the sum of all possible whole number values of n using Triangle Inequality?\"\n",
        "retrieved_contexts = retrieve(query, k=3)\n",
        "\n",
        "print(\"Retrieved Contexts:\")\n",
        "for context in retrieved_contexts:\n",
        "    print(context)\n",
        "\n",
        "query = \"What is the sum of all possible whole number values of n using Triangle Inequality?\"\n",
        "response = rag_pipeline(query)\n",
        "\n",
        "print(\"Generated Response:\")\n",
        "print(response)\n"
      ]
    }
  ]
}
