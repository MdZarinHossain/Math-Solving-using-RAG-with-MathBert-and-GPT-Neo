!pip install langchain openai faiss-cpu sentence-transformers transformers tiktoken langchain-community

import os
import json
import faiss
import numpy as np
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser

from google.colab import drive
drive.mount('/content/drive')

base_dir = "/content/drive/MyDrive/Capstone/MATH/train"

import glob
json_files = glob.glob(base_dir + "/*/*.json")
print(f"Total JSON files: {len(json_files)}")

for file_path in json_files[:5]:
    print(file_path)

import re
from sympy import sympify

def split_content(content):
    math_pattern = r"(\$.*?\$)"
    text_parts = re.split(math_pattern, content)
    chunks = []
    for part in text_parts:
        if re.match(math_pattern, part):
            chunks.append({"type": "math", "content": part})
        elif part.strip():
            chunks.append({"type": "text", "content": part.strip()})
    return chunks

def split_text_chunks(content, chunk_size=500, chunk_overlap=50):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    return text_splitter.split_text(content)

!pip install transformers torch

from transformers import AutoTokenizer, AutoModel
import torch

mathbert_model_name = "tbs17/MathBERT"
tokenizer_mathbert = AutoTokenizer.from_pretrained(mathbert_model_name)
tokenizer_mathbert.pad_token = tokenizer_mathbert.eos_token
model_mathbert = AutoModel.from_pretrained(mathbert_model_name)

def generate_embedding_mathbert(content):
    inputs = tokenizer_mathbert(content, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model_mathbert(**inputs)
    embedding = outputs.last_hidden_state[:, 0, :].numpy()  
    return embedding

import faiss
import numpy as np
import glob
import json

dimension = 768
faiss_index = faiss.IndexFlatL2(dimension)
embedding_map = {}


for json_file in json_files:
    with open(json_file, 'r') as f:
        data = json.load(f)

    if not isinstance(data, dict):
        print(f"Error: {json_file} does not contain a valid dictionary. Skipping.")
        continue

    problem = data.get("problem", "No problem found")
    solution = data.get("solution", "No solution found")

    if problem and solution:
        print(f"Processing file: {json_file}")
        print(f"Problem: {problem}")
        print(f"Solution: {solution}")

        problem_embedding = generate_embedding_mathbert(problem)
        solution_embedding = generate_embedding_mathbert(solution)

        faiss_index.add(np.array([problem_embedding]))
        embedding_map[len(embedding_map)] = {"type": "problem", "content": problem}

        faiss_index.add(np.array([solution_embedding]))
        embedding_map[len(embedding_map)] = {"type": "solution", "content": solution}

print(f"FAISS index contains {len(embedding_map)} embeddings.")

num_embeddings_faiss = faiss_index.ntotal
print(f"Total embeddings in FAISS: {num_embeddings_faiss}")

num_embeddings_map = len(embedding_map)
print(f"Total entries in embedding map: {num_embeddings_map}")

expected_count = 7500 * 2
if num_embeddings_faiss == expected_count and num_embeddings_map == expected_count:
    print("All files successfully processed and indexed!")
else:
    print(f"Discrepancy detected. Expected: {expected_count}, Found: {num_embeddings_faiss}")

import pickle
import faiss

faiss.write_index(faiss_index, "faiss_index.idx")

with open("embedding_map.pkl", "wb") as f:
    pickle.dump(embedding_map, f)

print("FAISS index and embedding map saved!")

!cp /content/faiss_index.idx /content/drive/MyDrive/Capstone/
!cp /content/embedding_map.pkl /content/drive/MyDrive/Capstone/


from google.colab import drive
drive.mount('/content/drive')

import pickle
import faiss

faiss_index = faiss.read_index("/content/drive/MyDrive/Capstone/faiss_index.idx")

with open("/content/drive/MyDrive/Capstone/embedding_map.pkl", "rb") as f:
    embedding_map = pickle.load(f)

print("FAISS index and embedding map loaded successfully!")

print(f"Total embeddings in FAISS: {faiss_index.ntotal}")
print(f"Total entries in embedding map: {len(embedding_map)}")

pip install transformers accelerate

from google.colab import userdata
userdata.get('Hugging_face')

!pip install --upgrade transformers


from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel
import numpy as np
import faiss
from langchain.prompts import ChatPromptTemplate



gpt_neo_model_name = "EleutherAI/gpt-neo-1.3B"
tokenizer_gpt = AutoTokenizer.from_pretrained(gpt_neo_model_name)
tokenizer_gpt.pad_token = tokenizer_gpt.eos_token
gpt_neo_model = AutoModelForCausalLM.from_pretrained(gpt_neo_model_name)



def retrieve(query, k=3, max_context_length=1500):
    query_embedding = generate_embedding_mathbert(query)  
    distances, indices = faiss_index.search(np.array([query_embedding]), k)  
    results = [embedding_map[i] for i in indices[0]]

    combined_context = "\n".join([item["content"] for item in results])
    return combined_context[:max_context_length]

template = """
You are a math tutor. Use the following retrieved context to answer the question.
Context: {context}
Question: {question}
Answer:
"""
prompt = ChatPromptTemplate.from_template(template)

def rag_pipeline(query):
    # Retrieve relevant contexts
    retrieved_contexts = retrieve(query, k=3)
    context_text = "\n".join([retrieved_contexts])  

  
    input_text = prompt.format(context=context_text, question=query)
    print(f"Input text length (characters): {len(input_text)}")  

    inputs = tokenizer_gpt(
        input_text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=2048  
    )
    print(f"Tokenized input length (tokens): {inputs['input_ids'].shape[1]}")  

    outputs = gpt_neo_model.generate(
        **inputs,
        max_length=200,  
        temperature=0.7
    )

    return tokenizer_gpt.decode(outputs[0], skip_special_tokens=True)

query = "What is the sum of all possible whole number values of n using Triangle Inequality?"
retrieved_contexts = retrieve(query, k=3)

print("Retrieved Contexts:")
for context in retrieved_contexts:
    print(context)

query = "What is the sum of all possible whole number values of n using Triangle Inequality?"
response = rag_pipeline(query)

print("Generated Response:")
print(response)
